{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from decord import VideoReader\n",
    "from decord import cpu\n",
    "import pandas as pd\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 1280, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "model = models.efficientnet_v2_s(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# removing the classification to get the feature extractor\n",
    "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "feature_extractor.to(device)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "with torch.no_grad():\n",
    "    output = feature_extractor(dummy_input)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "# preprocessing for ENV2\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(  # these are from ImageNet\n",
    "            mean=[0.485, 0.456, 0.406],  # which is what the\n",
    "            std=[0.229, 0.224, 0.225],  # model was trained on\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "video_dir = \"../4_desynced/train_dist/\"\n",
    "output_dir = \"./train_dist/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "frame_rate = 15\n",
    "batch_size = 256\n",
    "use_autoencoder = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim=1280, compressed_dim=128):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, compressed_dim),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(compressed_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        compressed = self.encoder(x)\n",
    "        reconstructed = self.decoder(compressed)\n",
    "        return compressed, reconstructed\n",
    "\n",
    "\n",
    "# Adjust the feature extraction function to pass through the autoencoder\n",
    "def extract_features_with_autoencoder(\n",
    "    path, feature_extractor, preprocess, autoencoder, device, fps, batch_size=32\n",
    "):\n",
    "    frames = extract_frames(path, fps)\n",
    "    inputs = torch.stack([preprocess(frame) for frame in frames]).to(device)\n",
    "\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch = inputs[i : i + batch_size]\n",
    "            batch_features = feature_extractor(batch)\n",
    "            batch_features = batch_features.view(batch_features.size(0), -1)\n",
    "\n",
    "            # Pass through autoencoder to reduce dimensions\n",
    "            compressed_features, _ = autoencoder(batch_features)\n",
    "            features.append(compressed_features.cpu().numpy())\n",
    "\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_features(path, model, preprocess, device, fps, batch_size=32):\n",
    "    frames = extract_frames(path, fps)\n",
    "    inputs = torch.stack([preprocess(frame) for frame in frames]).to(device)\n",
    "\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch = inputs[i : i + batch_size]\n",
    "            batch_features = model(batch)\n",
    "\n",
    "            # flattening the features\n",
    "            batch_features = batch_features.view(batch_features.size(0), -1)\n",
    "            features.append(batch_features.cpu().numpy())\n",
    "\n",
    "    features = np.concatenate(features, axis=0)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def parse_filename(filename):\n",
    "    base_name = os.path.basename(filename)\n",
    "    parts = base_name.rsplit(\"_\")\n",
    "    # if there are more than 4 parts, it means the filename has underscores in it\n",
    "    # keeping only the last 4 parts\n",
    "    if len(parts) > 4:\n",
    "        non_id_parts = parts[-3:]\n",
    "        # then, take all but the last 3 parts as the video id\n",
    "        # and concatenate them together, adding back the _\n",
    "        video_id = \"_\".join(parts[:-3])\n",
    "        clip_num, d, desync = non_id_parts\n",
    "    elif len(parts) == 4:\n",
    "        # print(parts)\n",
    "        video_id, clip_num, d, desync = parts\n",
    "    desync = desync.split(\".\")[0]  # removing .mp4\n",
    "    return video_id, clip_num, desync\n",
    "\n",
    "\n",
    "def extract_frames(video_path, frame_rate):\n",
    "    try:\n",
    "        vr = VideoReader(video_path, ctx=cpu())\n",
    "        total_frames = len(vr)\n",
    "        fps = vr.get_avg_fps()\n",
    "        if fps == 0:\n",
    "            fps = 15\n",
    "        interval = int(fps / frame_rate)\n",
    "        frame_indices = list(range(0, total_frames, interval))\n",
    "        frames = vr.get_batch(frame_indices).asnumpy()\n",
    "        # Convert to PIL Images\n",
    "        frames = [transforms.ToPILImage()(frame) for frame in frames]\n",
    "        return frames\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting frames with decord from {video_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def write_features_to_csv(\n",
    "    csv_path, video_id, clip_num, features, desync, existing=False\n",
    "):\n",
    "    num_features = features.shape[1]\n",
    "    mode = \"a\" if existing else \"w\"\n",
    "    with open(csv_path, mode, newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not existing:\n",
    "            header = [\"video_id\", \"video_number\", \"frame_number\", \"desync\"] + [\n",
    "                f\"feature_{i}\" for i in range(num_features)\n",
    "            ]\n",
    "            writer.writerow(header)\n",
    "        for frame_num, feature in enumerate(features):\n",
    "            row = [video_id, clip_num, frame_num, desync] + feature.tolist()\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/2400 [00:17<57:37,  1.45s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# check if autoencoder exists\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m autoencoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_with_autoencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvid_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     feats \u001b[38;5;241m=\u001b[39m extract_features(\n\u001b[0;32m     31\u001b[0m         vid_path, feature_extractor, preprocess, device, frame_rate, batch_size\n\u001b[0;32m     32\u001b[0m     )\n",
      "Cell \u001b[1;32mIn [3], line 22\u001b[0m, in \u001b[0;36mextract_features_with_autoencoder\u001b[1;34m(path, feature_extractor, preprocess, autoencoder, device, fps, batch_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features_with_autoencoder\u001b[39m(path, feature_extractor, preprocess, autoencoder, device, fps, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m---> 22\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mextract_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([preprocess(frame) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m     features \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn [3], line 77\u001b[0m, in \u001b[0;36mextract_frames\u001b[1;34m(video_path, frame_rate)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_frames\u001b[39m(video_path, frame_rate):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m         vr \u001b[38;5;241m=\u001b[39m \u001b[43mVideoReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m         total_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vr)\n\u001b[0;32m     79\u001b[0m         fps \u001b[38;5;241m=\u001b[39m vr\u001b[38;5;241m.\u001b[39mget_avg_fps()\n",
      "File \u001b[1;32mc:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\decord\\video_reader.py:54\u001b[0m, in \u001b[0;36mVideoReader.__init__\u001b[1;34m(self, uri, ctx, width, height, num_threads, fault_tol)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _CAPI_VideoReaderGetVideoReader(\n\u001b[0;32m     52\u001b[0m         ba, ctx\u001b[38;5;241m.\u001b[39mdevice_type, ctx\u001b[38;5;241m.\u001b[39mdevice_id, width, height, num_threads, \u001b[38;5;241m2\u001b[39m, fault_tol)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_CAPI_VideoReaderGetVideoReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfault_tol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m uri \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\decord\\_ffi\\_ctypes\\function.py:173\u001b[0m, in \u001b[0;36mFunctionBase.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    171\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m DECORDValue()\n\u001b[0;32m    172\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m--> 173\u001b[0m check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDECORDFuncCall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_tcode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    176\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[0;32m    177\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if use_autoencoder:\n",
    "    autoencoder = Autoencoder(input_dim=1280, compressed_dim=128)\n",
    "    autoencoder.to(device)\n",
    "else:\n",
    "    autoencoder = None\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith(\".mp4\")]\n",
    "\n",
    "# going across all ids\n",
    "grouped_videos = defaultdict(list)\n",
    "for vf in video_files:\n",
    "    video_id, clip_num, desync = parse_filename(vf)\n",
    "    # print(video_id, clip_num, desync)\n",
    "    if video_id is not None:\n",
    "        grouped_videos[video_id].append((vf, clip_num, desync))\n",
    "\n",
    "# going over each group of clips for a given video_id\n",
    "for video_id, videos in tqdm(grouped_videos.items()):\n",
    "    all_features = []\n",
    "    for video_file, clip_num, desync in videos:\n",
    "        vid_path = os.path.join(video_dir, video_file)\n",
    "        # check if autoencoder exists\n",
    "        if autoencoder is not None:\n",
    "            feats = extract_features_with_autoencoder(\n",
    "                vid_path, feature_extractor, preprocess, autoencoder, device, frame_rate, batch_size\n",
    "            )\n",
    "        else:\n",
    "            feats = extract_features(\n",
    "                vid_path, feature_extractor, preprocess, device, frame_rate, batch_size\n",
    "            )\n",
    "\n",
    "        if feats is None:\n",
    "            print(f\"Error extracting features from: {vid_path}\")\n",
    "            continue\n",
    "\n",
    "        # Adding features and metadata\n",
    "        for frame_num, feature in enumerate(feats):\n",
    "            all_features.append(\n",
    "                [video_id, clip_num, frame_num, desync] + feature.tolist()\n",
    "            )\n",
    "\n",
    "    # Writing all clips to CSV at a time\n",
    "    if all_features:\n",
    "        columns = [\"video_id\", \"video_number\", \"frame_number\", \"desync\"] + [\n",
    "            f\"feature_{i}\" for i in range(len(all_features[0]) - 4)\n",
    "        ]\n",
    "        df = pd.DataFrame(all_features, columns=columns)\n",
    "        parquet_path = os.path.join(output_dir, f\"{video_id}.parquet\")\n",
    "        df.to_parquet(parquet_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the avg, std, min, and max file sizes\n",
    "file_sizes = []\n",
    "for root, dirs, files in os.walk(output_dir):\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(root, file)))\n",
    "\n",
    "print(f\"Average file size: {np.mean(file_sizes)}\")\n",
    "print(f\"Std file size: {np.std(file_sizes)}\")\n",
    "print(f\"Min file size: {np.min(file_sizes)}\")\n",
    "print(f\"Max file size: {np.max(file_sizes)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
